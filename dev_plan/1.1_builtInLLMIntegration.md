# 1.1 Built-in LLM Integration Development Plan

## ðŸŽ¯ Objective
Add native built-in support for three major LLM providers to Onyx:
- **Ollama** (local/self-hosted models)
- **Groq** (fast inference cloud API) 
- **Gemini** (Google's AI models via direct API)

## ðŸ“‹ Current Architecture Analysis

### Existing LLM Provider System
**File**: `backend/onyx/llm/llm_provider_options.py`

**Current Built-in Providers:**
1. âœ… **OpenAI** - `openai` provider with GPT models
2. âœ… **Anthropic** - `anthropic` provider with Claude models  
3. âœ… **Azure OpenAI** - `azure` provider with deployment support
4. âœ… **AWS Bedrock** - `bedrock` provider with IAM auth
5. âœ… **GCP Vertex AI** - `vertex_ai` provider with **Gemini models already supported**

### Key Architecture Components

**1. Provider Definition System**
```python
class WellKnownLLMProviderDescriptor(BaseModel):
    name: str                          # Provider identifier
    display_name: str                  # UI display name  
    api_key_required: bool             # Authentication requirement
    api_base_required: bool            # Custom endpoint support
    api_version_required: bool         # API version requirement
    custom_config_keys: list[CustomConfigKey]  # Additional config fields
    model_configurations: list[ModelConfigurationView]  # Available models
    default_model: str | None          # Primary model recommendation
    default_fast_model: str | None     # Fast/cheap model recommendation
```

**2. Model Configuration System**
- Models defined per provider in `_PROVIDER_TO_MODELS_MAP`
- Visible models (UI-exposed) in `_PROVIDER_TO_VISIBLE_MODELS_MAP` 
- Image support detection via `model_supports_image_input()`

**3. Factory Pattern** (`factory.py`)
- LLM instantiation through `get_llms_for_persona()`
- Provider-specific kwargs via `_build_extra_model_kwargs()`
- **Already has Ollama support**: `{"num_ctx": GEN_AI_MODEL_FALLBACK_MAX_TOKENS} if provider == "ollama"`

## ðŸ” Implementation Analysis

### Status Assessment

**âœ… Gemini**: **ALREADY IMPLEMENTED** via Vertex AI provider
- Models: `gemini-2.0-flash`, `gemini-1.5-pro`, `gemini-1.5-flash`, etc.
- Authentication: GCP service account JSON file
- Status: **COMPLETE** - no additional work needed

**ðŸŸ¡ Ollama**: **PARTIALLY IMPLEMENTED**
- Factory support: âœ… Special kwargs handling exists
- Provider definition: âŒ Missing from `fetch_available_well_known_llms()`
- Model definitions: âŒ No model list defined
- Status: **NEEDS COMPLETION**

**âŒ Groq**: **NOT IMPLEMENTED**
- No existing traces in codebase
- Status: **FULL IMPLEMENTATION REQUIRED**

## ðŸš€ Implementation Plan

### Phase 1: Ollama Integration Completion

**1.1 Add Ollama Provider Definition**
- Location: `backend/onyx/llm/llm_provider_options.py`
- Add `OLLAMA_PROVIDER_NAME = "ollama"`  
- Define `OLLAMA_MODEL_NAMES` list with popular models
- Create `WellKnownLLMProviderDescriptor` for Ollama

**1.2 Ollama Configuration**
- API key: Not required (local deployment)
- API base: Required (e.g., `http://localhost:11434`)
- Custom config: Ollama endpoint URL
- Default models: `llama3.2:latest`, `qwen2.5:latest`

**1.3 Model List Research**
Research popular/stable Ollama models:
- Llama family: `llama3.2:1b`, `llama3.2:3b`, `llama3.2:7b`
- Qwen family: `qwen2.5:0.5b`, `qwen2.5:1.5b`, `qwen2.5:7b`
- Code models: `deepseek-coder`, `codellama`
- Specialized: `phi3.5:mini`, `mistral-nemo`

### Phase 2: Groq Integration (Full Implementation)

**2.1 Add Groq Provider Definition**
- Location: `backend/onyx/llm/llm_provider_options.py`
- Add `GROQ_PROVIDER_NAME = "groq"`
- Define `GROQ_MODEL_NAMES` list
- Create `WellKnownLLMProviderDescriptor` for Groq

**2.2 Groq Configuration**
- API key: Required (`GROQ_API_KEY`)
- API base: Optional (defaults to Groq's endpoint)
- Models: Focus on Groq's fastest models
- Default models: `llama-3.1-8b-instant`, `mixtral-8x7b-32768`

**2.3 Groq Model Research**
Current Groq popular models:
- **Llama**: `llama-3.1-8b-instant`, `llama-3.1-70b-versatile`
- **Mixtral**: `mixtral-8x7b-32768`  
- **Gemma**: `gemma2-9b-it`
- **Other**: Check latest via Groq API/docs

### Phase 3: Integration & Testing

**3.1 LiteLL intarface Verification** 
- Verify LiteLLM supports Ollama: âœ… (already has special handling)
- Verify LiteLLM supports Groq: Research required
- Test model name formats and authentication

**3.2 Configuration Testing**
- Test API key validation
- Test endpoint connectivity  
- Test model listing and selection
- Test image support detection (if applicable)

**3.3 UI Integration Testing**
- Verify providers appear in admin interface
- Test model selection dropdowns
- Test configuration form validation
- Test default model assignment

## ðŸ“ Files to Modify

### Primary Implementation Files
1. **`backend/onyx/llm/llm_provider_options.py`**
   - Add Ollama and Groq provider definitions
   - Add model name lists
   - Add provider descriptors to `fetch_available_well_known_llms()`

2. **`backend/onyx/llm/factory.py`** 
   - Potentially add Groq-specific kwargs (if needed)
   - Existing Ollama support should work as-is

3. **`backend/onyx/llm/utils.py`**
   - May need model support detection updates
   - Verify image support logic for new models

### Configuration Files
4. **`backend/.env.dev`** (for testing)
   - Add sample Groq API key configuration
   - Add sample Ollama endpoint configuration

### Documentation Files  
5. **`DEVELOPMENT.md`** (update LLM configuration section)
6. **New**: `docs/llm_providers.md` (comprehensive provider guide)

## ðŸ§ª Testing Strategy

### Unit Testing
- Test provider descriptor creation
- Test model name validation  
- Test configuration key validation
- Test authentication parameter handling

### Integration Testing  
- **Ollama**: Requires local Ollama installation
- **Groq**: Requires valid Groq API key
- **Gemini**: Already tested via Vertex AI

### UI Testing
- Admin interface provider selection
- Model dropdown population
- Configuration form validation
- Error handling for invalid credentials

## ðŸ“Š Success Criteria

**Ollama Integration:**
- âœ… Provider appears in admin interface
- âœ… Models populate correctly in dropdowns  
- âœ… Local Ollama endpoint connection works
- âœ… Text generation functional via chat interface

**Groq Integration:**
- âœ… Provider appears in admin interface
- âœ… Models populate correctly in dropdowns
- âœ… API key authentication works  
- âœ… Fast inference speeds demonstrated
- âœ… Text generation functional via chat interface

**General:**
- âœ… No regression in existing providers
- âœ… Configuration persistence works
- âœ… Error handling graceful for network issues
- âœ… Documentation updated

## ðŸš§ Implementation Dependencies

### External Dependencies
- **Ollama**: Requires Ollama server installation for testing
- **Groq**: Requires Groq API account and key
- **LiteLLM**: Verify latest version supports all target models

### Internal Dependencies
- Current LLM factory architecture (no changes needed)
- Existing provider management UI (should work automatically)
- Database schema for LLM provider storage (no changes needed)

## ðŸ“‹ Next Steps

1. **Research Phase**: Verify latest model names and API compatibility
2. **Ollama Completion**: Add missing provider definition  
3. **Groq Implementation**: Full provider implementation
4. **Testing**: Local testing with actual services
5. **Documentation**: Update configuration guides
6. **PR Submission**: Submit changes with comprehensive testing

## ðŸŽ¨ Enhanced UI/UX Architecture for Extensible LLM Providers

### Current UI Analysis

**Limitations of Current Architecture:**
1. **Hard-coded Provider List**: Only supports 5 built-in providers (OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, GCP Vertex AI)
2. **Static Form Design**: Each provider requires custom form implementation 
3. **Non-extensible**: Adding new providers requires code changes across multiple files
4. **Complex Configuration**: Users struggle with provider-specific setup (API keys, base URLs, custom config)
5. **Poor Discoverability**: LiteLLM supports 100+ providers but only 5 are easily accessible

### Extensible UI Architecture Design

**1. Dynamic Provider Registry**
```typescript
interface ProviderTemplate {
  id: string;                    // e.g. "groq", "ollama", "together_ai"
  name: string;                  // Display name
  description: string;           // Provider description
  category: "cloud" | "local" | "enterprise";
  logoUrl?: string;              // Provider logo
  documentation_url?: string;    // Link to provider docs
  setup_difficulty: "easy" | "medium" | "hard";
  
  // Configuration schema
  config_schema: {
    api_key?: FieldConfig;
    api_base?: FieldConfig;
    api_version?: FieldConfig;
    custom_fields?: FieldConfig[];
  };
  
  // Model information
  popular_models: string[];      // Pre-selected popular models
  model_fetching: "static" | "dynamic" | "manual";
  
  // LiteLLM integration
  litellm_provider_name: string; // Maps to LiteLLM provider
  model_prefix?: string;         // e.g. "groq/", "together/"
}

interface FieldConfig {
  type: "text" | "password" | "url" | "select" | "file" | "textarea";
  label: string;
  placeholder?: string;
  description?: string;
  required: boolean;
  validation?: string;           // Regex pattern
  options?: string[];           // For select fields
  default_value?: string;
}
```

**2. Provider Category System**
- **Popular Cloud Providers**: OpenAI, Anthropic, Groq, Together AI, Fireworks AI
- **Enterprise/Enterprise**: Azure OpenAI, AWS Bedrock, GCP Vertex AI, AWS Sagemaker
- **Local/Self-hosted**: Ollama, LM Studio, LocalAI, Text Generation Inference
- **Specialized**: Cohere, AI21, Replicate, Anyscale, Perplexity AI

**3. Smart Configuration Wizard**
```typescript
interface ConfigWizard {
  steps: [
    {
      title: "Choose Provider";
      categories: ["popular", "enterprise", "local", "advanced"];
      search_enabled: true;
      recommendations: boolean;  // Based on use case
    },
    {
      title: "Basic Configuration";
      dynamic_form: true;        // Generated from ProviderTemplate
      validation: "real_time";
      help_links: true;
    },
    {
      title: "Model Selection";
      model_discovery: "auto" | "manual";
      popular_first: true;
      performance_hints: boolean;
    },
    {
      title: "Test Connection";
      test_request: boolean;
      validation_model: string;
      success_callback: () => void;
    }
  ];
}
```

**4. Enhanced UI Components**

**Provider Card Component:**
```tsx
interface ProviderCardProps {
  provider: ProviderTemplate;
  onSelect: (provider: ProviderTemplate) => void;
  isPopular?: boolean;
  difficulty: "easy" | "medium" | "hard";
}

// Features:
// - Provider logo and branding
// - Setup difficulty indicator
// - Popular/recommended badges  
// - Documentation link
// - Quick setup preview
```

**Dynamic Form Generator:**
```tsx
interface DynamicFormProps {
  schema: ProviderTemplate['config_schema'];
  onValidate: (isValid: boolean) => void;
  onSubmit: (config: any) => Promise<void>;
  testConnection?: boolean;
}

// Features:
// - Real-time validation
// - Smart field suggestions
// - Context-aware help
// - Auto-fill common values
// - Connection testing
```

**Model Discovery Component:**
```tsx
interface ModelDiscoveryProps {
  provider: string;
  apiConfig: any;
  onModelsSelected: (models: string[]) => void;
  mode: "auto" | "manual" | "hybrid";
}

// Features:
// - Auto-fetch models from provider API
// - Popular models pre-selection
// - Model performance indicators
// - Custom model input
```

### Implementation Phases

**Phase 1: Provider Template System**
- Create extensible provider templates for all LiteLLM providers
- Build provider registry with categorization
- Implement dynamic form generation system

**Phase 2: Enhanced UI Components**  
- Redesign provider selection with search and categories
- Build smart configuration wizard with validation
- Implement real-time connection testing

**Phase 3: LiteLLM Integration**
- Map all provider templates to LiteLLM provider names
- Implement automatic model discovery where possible
- Add provider-specific optimization hints

**Phase 4: Advanced Features**
- Provider recommendation engine
- Configuration templates for common use cases
- Bulk provider setup for enterprises
- Provider health monitoring and alerts

### Provider Template Definitions

**Groq Provider Template:**
```yaml
id: "groq"
name: "Groq"
description: "Ultra-fast inference with Groq's LPU technology"
category: "cloud" 
setup_difficulty: "easy"
documentation_url: "https://groq.com/docs/"

config_schema:
  api_key:
    type: "password"
    label: "Groq API Key"
    placeholder: "gsk_..."
    required: true
    description: "Get your API key from console.groq.com"

popular_models:
  - "llama-3.1-8b-instant"
  - "llama-3.1-70b-versatile"
  - "mixtral-8x7b-32768"
  - "gemma2-9b-it"

litellm_provider_name: "groq"
model_prefix: "groq/"
```

**Ollama Provider Template:**
```yaml  
id: "ollama"
name: "Ollama"
description: "Run LLMs locally on your machine"
category: "local"
setup_difficulty: "medium"
documentation_url: "https://ollama.ai/docs/"

config_schema:
  api_base:
    type: "url"
    label: "Ollama Server URL"
    placeholder: "http://localhost:11434"
    required: true
    default_value: "http://localhost:11434"
    description: "URL of your Ollama server"

popular_models:
  - "llama3.2:latest"
  - "qwen2.5:latest" 
  - "deepseek-coder:latest"
  - "mistral-nemo:latest"

model_fetching: "dynamic"  # Can fetch from /api/tags
litellm_provider_name: "ollama"
```

**Together AI Provider Template:**
```yaml
id: "together"
name: "Together AI"  
description: "Fast inference for open-source models"
category: "cloud"
setup_difficulty: "easy"

config_schema:
  api_key:
    type: "password" 
    label: "Together AI API Key"
    required: true

popular_models:
  - "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
  - "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"

litellm_provider_name: "together_ai"
model_prefix: "together_ai/"
```

### Benefits of New Architecture

**For Users:**
- **Easier Discovery**: Browse 100+ providers by category 
- **Guided Setup**: Step-by-step wizard with validation
- **Better Documentation**: Contextual help and links
- **Faster Configuration**: Smart defaults and templates

**For Developers:**
- **Extensible**: Add new providers without code changes
- **Maintainable**: Centralized provider definitions
- **Testable**: Standardized configuration schema
- **Scalable**: Support for hundreds of providers

**For Enterprise:**
- **Bulk Setup**: Configure multiple providers at once
- **Compliance**: Built-in validation and security checks
- **Monitoring**: Provider health and performance tracking
- **Templates**: Pre-configured setups for common use cases

## ðŸ’¡ Updated Implementation Notes

- **Gemini**: Already fully supported via Vertex AI - no work needed
- **Ollama**: 70% complete - implement with new extensible UI
- **Groq**: Full implementation using new provider template system
- **LiteLLM**: Leverage all 100+ supported providers
- **UI Architecture**: Complete redesign for extensibility and UX
- **Backward Compatibility**: Existing providers work unchanged