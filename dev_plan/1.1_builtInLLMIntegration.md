# 1.1 Built-in LLM Integration Development Plan

## üéØ Objective
Add native built-in support for three major LLM providers to Onyx:
- **Ollama** (local/self-hosted models)
- **Groq** (fast inference cloud API) 
- **Gemini** (Google's AI models via direct API)

## üìã Current Architecture Analysis

### Existing LLM Provider System
**File**: `backend/onyx/llm/llm_provider_options.py`

**Current Built-in Providers:**
1. ‚úÖ **OpenAI** - `openai` provider with GPT models
2. ‚úÖ **Anthropic** - `anthropic` provider with Claude models  
3. ‚úÖ **Azure OpenAI** - `azure` provider with deployment support
4. ‚úÖ **AWS Bedrock** - `bedrock` provider with IAM auth
5. ‚úÖ **GCP Vertex AI** - `vertex_ai` provider with **Gemini models already supported**

### Key Architecture Components

**1. Provider Definition System**
```python
class WellKnownLLMProviderDescriptor(BaseModel):
    name: str                          # Provider identifier
    display_name: str                  # UI display name  
    api_key_required: bool             # Authentication requirement
    api_base_required: bool            # Custom endpoint support
    api_version_required: bool         # API version requirement
    custom_config_keys: list[CustomConfigKey]  # Additional config fields
    model_configurations: list[ModelConfigurationView]  # Available models
    default_model: str | None          # Primary model recommendation
    default_fast_model: str | None     # Fast/cheap model recommendation
```

**2. Model Configuration System**
- Models defined per provider in `_PROVIDER_TO_MODELS_MAP`
- Visible models (UI-exposed) in `_PROVIDER_TO_VISIBLE_MODELS_MAP` 
- Image support detection via `model_supports_image_input()`

**3. Factory Pattern** (`factory.py`)
- LLM instantiation through `get_llms_for_persona()`
- Provider-specific kwargs via `_build_extra_model_kwargs()`
- **Already has Ollama support**: `{"num_ctx": GEN_AI_MODEL_FALLBACK_MAX_TOKENS} if provider == "ollama"`

## üîç Implementation Analysis

### Status Assessment

**‚úÖ Gemini**: **ALREADY IMPLEMENTED** via Vertex AI provider
- Models: `gemini-2.0-flash`, `gemini-1.5-pro`, `gemini-1.5-flash`, etc.
- Authentication: GCP service account JSON file
- Status: **COMPLETE** - no additional work needed

**üü° Ollama**: **PARTIALLY IMPLEMENTED**
- Factory support: ‚úÖ Special kwargs handling exists
- Provider definition: ‚ùå Missing from `fetch_available_well_known_llms()`
- Model definitions: ‚ùå No model list defined
- Status: **NEEDS COMPLETION**

**‚ùå Groq**: **NOT IMPLEMENTED**
- No existing traces in codebase
- Status: **FULL IMPLEMENTATION REQUIRED**

## üöÄ Implementation Plan

### Phase 1: Ollama Integration Completion

**1.1 Add Ollama Provider Definition**
- Location: `backend/onyx/llm/llm_provider_options.py`
- Add `OLLAMA_PROVIDER_NAME = "ollama"`  
- Define `OLLAMA_MODEL_NAMES` list with popular models
- Create `WellKnownLLMProviderDescriptor` for Ollama

**1.2 Ollama Configuration**
- API key: Not required (local deployment)
- API base: Required (e.g., `http://localhost:11434`)
- Custom config: Ollama endpoint URL
- Default models: `llama3.2:latest`, `qwen2.5:latest`

**1.3 Model List Research**
Research popular/stable Ollama models:
- Llama family: `llama3.2:1b`, `llama3.2:3b`, `llama3.2:7b`
- Qwen family: `qwen2.5:0.5b`, `qwen2.5:1.5b`, `qwen2.5:7b`
- Code models: `deepseek-coder`, `codellama`
- Specialized: `phi3.5:mini`, `mistral-nemo`

### Phase 2: Groq Integration (Full Implementation)

**2.1 Add Groq Provider Definition**
- Location: `backend/onyx/llm/llm_provider_options.py`
- Add `GROQ_PROVIDER_NAME = "groq"`
- Define `GROQ_MODEL_NAMES` list
- Create `WellKnownLLMProviderDescriptor` for Groq

**2.2 Groq Configuration**
- API key: Required (`GROQ_API_KEY`)
- API base: Optional (defaults to Groq's endpoint)
- Models: Focus on Groq's fastest models
- Default models: `llama-3.1-8b-instant`, `mixtral-8x7b-32768`

**2.3 Groq Model Research**
Current Groq popular models:
- **Llama**: `llama-3.1-8b-instant`, `llama-3.1-70b-versatile`
- **Mixtral**: `mixtral-8x7b-32768`  
- **Gemma**: `gemma2-9b-it`
- **Other**: Check latest via Groq API/docs

### Phase 3: Integration & Testing

**3.1 LiteLL intarface Verification** 
- Verify LiteLLM supports Ollama: ‚úÖ (already has special handling)
- Verify LiteLLM supports Groq: Research required
- Test model name formats and authentication

**3.2 Configuration Testing**
- Test API key validation
- Test endpoint connectivity  
- Test model listing and selection
- Test image support detection (if applicable)

**3.3 UI Integration Testing**
- Verify providers appear in admin interface
- Test model selection dropdowns
- Test configuration form validation
- Test default model assignment

## üìÅ Files to Modify

### Primary Implementation Files
1. **`backend/onyx/llm/llm_provider_options.py`**
   - Add Ollama and Groq provider definitions
   - Add model name lists
   - Add provider descriptors to `fetch_available_well_known_llms()`

2. **`backend/onyx/llm/factory.py`** 
   - Potentially add Groq-specific kwargs (if needed)
   - Existing Ollama support should work as-is

3. **`backend/onyx/llm/utils.py`**
   - May need model support detection updates
   - Verify image support logic for new models

### Configuration Files
4. **`backend/.env.dev`** (for testing)
   - Add sample Groq API key configuration
   - Add sample Ollama endpoint configuration

### Documentation Files  
5. **`DEVELOPMENT.md`** (update LLM configuration section)
6. **New**: `docs/llm_providers.md` (comprehensive provider guide)

## üß™ Testing Strategy

### Unit Testing
- Test provider descriptor creation
- Test model name validation  
- Test configuration key validation
- Test authentication parameter handling

### Integration Testing  
- **Ollama**: Requires local Ollama installation
- **Groq**: Requires valid Groq API key
- **Gemini**: Already tested via Vertex AI

### UI Testing
- Admin interface provider selection
- Model dropdown population
- Configuration form validation
- Error handling for invalid credentials

## üìä Success Criteria

**Ollama Integration:**
- ‚úÖ Provider appears in admin interface
- ‚úÖ Models populate correctly in dropdowns  
- ‚úÖ Local Ollama endpoint connection works
- ‚úÖ Text generation functional via chat interface

**Groq Integration:**
- ‚úÖ Provider appears in admin interface
- ‚úÖ Models populate correctly in dropdowns
- ‚úÖ API key authentication works  
- ‚úÖ Fast inference speeds demonstrated
- ‚úÖ Text generation functional via chat interface

**General:**
- ‚úÖ No regression in existing providers
- ‚úÖ Configuration persistence works
- ‚úÖ Error handling graceful for network issues
- ‚úÖ Documentation updated

## üöß Implementation Dependencies

### External Dependencies
- **Ollama**: Requires Ollama server installation for testing
- **Groq**: Requires Groq API account and key
- **LiteLLM**: Verify latest version supports all target models

### Internal Dependencies
- Current LLM factory architecture (no changes needed)
- Existing provider management UI (should work automatically)
- Database schema for LLM provider storage (no changes needed)

## üìã Next Steps

1. **Research Phase**: Verify latest model names and API compatibility
2. **Ollama Completion**: Add missing provider definition  
3. **Groq Implementation**: Full provider implementation
4. **Testing**: Local testing with actual services
5. **Documentation**: Update configuration guides
6. **PR Submission**: Submit changes with comprehensive testing

## üí° Additional Notes

- **Gemini**: Already fully supported via Vertex AI - no work needed
- **Ollama**: 70% complete - just needs provider definition
- **Groq**: 0% complete - full implementation required  
- **LiteLLM**: Leverages existing integration - minimal custom code needed
- **Backward Compatibility**: No breaking changes to existing providers