# 1.1 Built-in LLM Integration Development Plan

## üéØ Objective
Add native built-in support for three major LLM providers to Onyx:
- **Ollama** (local/self-hosted models)
- **Groq** (fast inference cloud API) 
- **Gemini** (Google's AI models via direct API)

## üìã Current Architecture Analysis

### Existing LLM Provider System
**File**: `backend/onyx/llm/llm_provider_options.py`

**Current Built-in Providers:**
1. ‚úÖ **OpenAI** - `openai` provider with GPT models
2. ‚úÖ **Anthropic** - `anthropic` provider with Claude models  
3. ‚úÖ **Azure OpenAI** - `azure` provider with deployment support
4. ‚úÖ **AWS Bedrock** - `bedrock` provider with IAM auth
5. ‚úÖ **GCP Vertex AI** - `vertex_ai` provider with **Gemini models already supported**

### Key Architecture Components

**1. Provider Definition System**
```python
class WellKnownLLMProviderDescriptor(BaseModel):
    name: str                          # Provider identifier
    display_name: str                  # UI display name  
    api_key_required: bool             # Authentication requirement
    api_base_required: bool            # Custom endpoint support
    api_version_required: bool         # API version requirement
    custom_config_keys: list[CustomConfigKey]  # Additional config fields
    model_configurations: list[ModelConfigurationView]  # Available models
    default_model: str | None          # Primary model recommendation
    default_fast_model: str | None     # Fast/cheap model recommendation
```

**2. Model Configuration System**
- Models defined per provider in `_PROVIDER_TO_MODELS_MAP`
- Visible models (UI-exposed) in `_PROVIDER_TO_VISIBLE_MODELS_MAP` 
- Image support detection via `model_supports_image_input()`

**3. Factory Pattern** (`factory.py`)
- LLM instantiation through `get_llms_for_persona()`
- Provider-specific kwargs via `_build_extra_model_kwargs()`
- **Already has Ollama support**: `{"num_ctx": GEN_AI_MODEL_FALLBACK_MAX_TOKENS} if provider == "ollama"`

## üîç Implementation Analysis

### Status Assessment

**‚úÖ Gemini**: **ALREADY IMPLEMENTED** via Vertex AI provider
- Models: `gemini-2.0-flash`, `gemini-1.5-pro`, `gemini-1.5-flash`, etc.
- Authentication: GCP service account JSON file
- Status: **COMPLETE** - no additional work needed

**üü° Ollama**: **PARTIALLY IMPLEMENTED**
- Factory support: ‚úÖ Special kwargs handling exists
- Provider definition: ‚ùå Missing from `fetch_available_well_known_llms()`
- Model definitions: ‚ùå No model list defined
- Status: **NEEDS COMPLETION**

**‚ùå Groq**: **NOT IMPLEMENTED**
- No existing traces in codebase
- Status: **FULL IMPLEMENTATION REQUIRED**

## üöÄ Implementation Plan

### Phase 1: Ollama Integration Completion

**1.1 Add Ollama Provider Definition**
- Location: `backend/onyx/llm/llm_provider_options.py`
- Add `OLLAMA_PROVIDER_NAME = "ollama"`  
- Define `OLLAMA_MODEL_NAMES` list with popular models
- Create `WellKnownLLMProviderDescriptor` for Ollama

**1.2 Ollama Configuration**
- API key: Not required (local deployment)
- API base: Required (e.g., `http://localhost:11434`)
- Custom config: Ollama endpoint URL
- Default models: `llama3.2:latest`, `qwen2.5:latest`

**1.3 Model List Research**
Research popular/stable Ollama models:
- Llama family: `llama3.2:1b`, `llama3.2:3b`, `llama3.2:7b`
- Qwen family: `qwen2.5:0.5b`, `qwen2.5:1.5b`, `qwen2.5:7b`
- Code models: `deepseek-coder`, `codellama`
- Specialized: `phi3.5:mini`, `mistral-nemo`

### Phase 2: Groq Integration (Full Implementation)

**2.1 Add Groq Provider Definition**
- Location: `backend/onyx/llm/llm_provider_options.py`
- Add `GROQ_PROVIDER_NAME = "groq"`
- Define `GROQ_MODEL_NAMES` list
- Create `WellKnownLLMProviderDescriptor` for Groq

**2.2 Groq Configuration**
- API key: Required (`GROQ_API_KEY`)
- API base: Optional (defaults to Groq's endpoint)
- Models: Focus on Groq's fastest models
- Default models: `llama-3.1-8b-instant`, `mixtral-8x7b-32768`

**2.3 Groq Model Research**
Current Groq popular models:
- **Llama**: `llama-3.1-8b-instant`, `llama-3.1-70b-versatile`
- **Mixtral**: `mixtral-8x7b-32768`  
- **Gemma**: `gemma2-9b-it`
- **Other**: Check latest via Groq API/docs

### Phase 3: Integration & Testing

**3.1 LiteLL intarface Verification** 
- Verify LiteLLM supports Ollama: ‚úÖ (already has special handling)
- Verify LiteLLM supports Groq: Research required
- Test model name formats and authentication

**3.2 Configuration Testing**
- Test API key validation
- Test endpoint connectivity  
- Test model listing and selection
- Test image support detection (if applicable)

**3.3 UI Integration Testing**
- Verify providers appear in admin interface
- Test model selection dropdowns
- Test configuration form validation
- Test default model assignment

## üìÅ Files to Modify

### Primary Implementation Files
1. **`backend/onyx/llm/llm_provider_options.py`**
   - Add Ollama and Groq provider definitions
   - Add model name lists
   - Add provider descriptors to `fetch_available_well_known_llms()`

2. **`backend/onyx/llm/factory.py`** 
   - Potentially add Groq-specific kwargs (if needed)
   - Existing Ollama support should work as-is

3. **`backend/onyx/llm/utils.py`**
   - May need model support detection updates
   - Verify image support logic for new models

### Configuration Files
4. **`backend/.env.dev`** (for testing)
   - Add sample Groq API key configuration
   - Add sample Ollama endpoint configuration

### Documentation Files  
5. **`DEVELOPMENT.md`** (update LLM configuration section)
6. **New**: `docs/llm_providers.md` (comprehensive provider guide)

## üß™ Testing Strategy

### Unit Testing
- Test provider descriptor creation
- Test model name validation  
- Test configuration key validation
- Test authentication parameter handling

### Integration Testing  
- **Ollama**: Requires local Ollama installation
- **Groq**: Requires valid Groq API key
- **Gemini**: Already tested via Vertex AI

### UI Testing
- Admin interface provider selection
- Model dropdown population
- Configuration form validation
- Error handling for invalid credentials

## üìä Success Criteria

**Ollama Integration:**
- ‚úÖ Provider appears in admin interface
- ‚úÖ Models populate correctly in dropdowns  
- ‚úÖ Local Ollama endpoint connection works
- ‚úÖ Text generation functional via chat interface

**Groq Integration:**
- ‚úÖ Provider appears in admin interface
- ‚úÖ Models populate correctly in dropdowns
- ‚úÖ API key authentication works  
- ‚úÖ Fast inference speeds demonstrated
- ‚úÖ Text generation functional via chat interface

**General:**
- ‚úÖ No regression in existing providers
- ‚úÖ Configuration persistence works
- ‚úÖ Error handling graceful for network issues
- ‚úÖ Documentation updated

## üöß Implementation Dependencies

### External Dependencies
- **Ollama**: Requires Ollama server installation for testing
- **Groq**: Requires Groq API account and key
- **LiteLLM**: Verify latest version supports all target models

### Internal Dependencies
- Current LLM factory architecture (no changes needed)
- Existing provider management UI (should work automatically)
- Database schema for LLM provider storage (no changes needed)

## üìã Next Steps

1. **Research Phase**: Verify latest model names and API compatibility
2. **Ollama Completion**: Add missing provider definition  
3. **Groq Implementation**: Full provider implementation
4. **Testing**: Local testing with actual services
5. **Documentation**: Update configuration guides
6. **PR Submission**: Submit changes with comprehensive testing

## üé® Enhanced UI/UX Architecture for Extensible LLM Providers

### Current UI Analysis

**Limitations of Current Architecture:**
1. **Hard-coded Provider List**: Only supports 5 built-in providers (OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, GCP Vertex AI)
2. **Static Form Design**: Each provider requires custom form implementation 
3. **Non-extensible**: Adding new providers requires code changes across multiple files
4. **Complex Configuration**: Users struggle with provider-specific setup (API keys, base URLs, custom config)
5. **Poor Discoverability**: LiteLLM supports 100+ providers but only 5 are easily accessible

### Extensible UI Architecture Design

**1. Dynamic Provider Registry**
```typescript
interface ProviderTemplate {
  id: string;                    // e.g. "groq", "ollama", "together_ai"
  name: string;                  // Display name
  description: string;           // Provider description
  category: "cloud" | "local" | "enterprise";
  logoUrl?: string;              // Provider logo
  documentation_url?: string;    // Link to provider docs
  setup_difficulty: "easy" | "medium" | "hard";
  
  // Configuration schema
  config_schema: {
    api_key?: FieldConfig;
    api_base?: FieldConfig;
    api_version?: FieldConfig;
    custom_fields?: FieldConfig[];
  };
  
  // Model information  
  popular_models?: string[];     // Optional fallback models for initial display
  model_fetching: "dynamic" | "static" | "manual";
  model_endpoint?: string;       // API endpoint to fetch available models
  model_list_cache_ttl?: number; // Cache time-to-live in seconds
  
  // LiteLLM integration
  litellm_provider_name: string; // Maps to LiteLLM provider
  model_prefix?: string;         // e.g. "groq/", "together/"
}

interface FieldConfig {
  type: "text" | "password" | "url" | "select" | "file" | "textarea";
  label: string;
  placeholder?: string;
  description?: string;
  required: boolean;
  validation?: string;           // Regex pattern
  options?: string[];           // For select fields
  default_value?: string;
}
```

**2. Provider Category System**
- **Popular Cloud**: OpenAI, Anthropic, Groq, Together AI, Fireworks AI
- **Enterprise**: Azure OpenAI, AWS Bedrock, GCP Vertex AI, AWS Sagemaker
- **Local/Self-hosted**: Ollama, LM Studio, LocalAI, Text Generation Inference
- **Specialized**: Cohere, AI21, Replicate, Anyscale, Perplexity AI
- **Custom Integration**: Manual LiteLLM provider configuration (preserved)

**3. Smart Configuration Wizard**
```typescript
interface ConfigWizard {
  steps: [
    {
      title: "Choose Provider";
      categories: ["popular", "enterprise", "local", "advanced"];
      search_enabled: true;
      recommendations: boolean;  // Based on use case
    },
    {
      title: "Basic Configuration";
      dynamic_form: true;        // Generated from ProviderTemplate
      validation: "real_time";
      help_links: true;
    },
    {
      title: "Model Selection";
      model_discovery: "dynamic" | "static" | "manual";
      dynamic_fetching: boolean;     // Fetch models from provider API
      fallback_models: string[];     // Use if API fetch fails
      cache_models: boolean;         // Cache fetched models
      performance_hints: boolean;
    },
    {
      title: "Test Connection";
      test_request: boolean;
      validation_model: string;
      success_callback: () => void;
    }
  ];
}
```

**4. Enhanced UI Components**

**Provider Card Component:**
```tsx
interface ProviderCardProps {
  provider: ProviderTemplate;
  onSelect: (provider: ProviderTemplate) => void;
  isPopular?: boolean;
  difficulty: "easy" | "medium" | "hard";
}

// Features:
// - Provider logo and branding
// - Setup difficulty indicator
// - Popular/recommended badges  
// - Documentation link
// - Quick setup preview
```

**Dynamic Form Generator:**
```tsx
interface DynamicFormProps {
  schema: ProviderTemplate['config_schema'];
  onValidate: (isValid: boolean) => void;
  onSubmit: (config: any) => Promise<void>;
  testConnection?: boolean;
}

// Features:
// - Real-time validation
// - Smart field suggestions
// - Context-aware help
// - Auto-fill common values
// - Connection testing
```

**Model Discovery Component:**
```tsx
interface ModelDiscoveryProps {
  provider: string;
  apiConfig: any;
  onModelsSelected: (models: string[]) => void;
  mode: "auto" | "manual" | "hybrid";
}

// Features:
// - Auto-fetch models from provider API
// - Popular models pre-selection
// - Model performance indicators
// - Custom model input
```

### Implementation Phases

**Phase 1: Core Infrastructure (Priority)**
- Implement provider template system for 4 target providers
- Create dynamic model fetching with caching
- Build extensible UI components (provider cards, wizard)
- Preserve existing custom integration feature

**Phase 2: Priority Provider Integration**
- **Ollama**: Complete missing provider definition 
- **Groq**: Full integration with dynamic model fetching
- **Together AI**: Complete integration setup
- **Fireworks AI**: Add as fourth priority provider

**Phase 3: Enhanced User Experience**
- Smart configuration wizard with step-by-step setup
- Provider categorization and search functionality  
- Real-time model availability validation
- Comprehensive error handling and fallbacks

**Phase 4: Future Extensibility** 
- Template system ready for additional providers
- Custom integration preserved for LiteLLM additions
- Monitoring and health check framework
- Enterprise features (bulk setup, compliance)

### Provider Template Definitions

**Groq Provider Template:**
```yaml
id: "groq"
name: "Groq"
description: "Ultra-fast inference with Groq's LPU technology"
category: "cloud" 
setup_difficulty: "easy"
documentation_url: "https://groq.com/docs/"

config_schema:
  api_key:
    type: "password"
    label: "Groq API Key"
    placeholder: "gsk_..."
    required: true
    description: "Get your API key from console.groq.com"

model_fetching: "dynamic"
model_endpoint: "https://api.groq.com/openai/v1/models"
model_list_cache_ttl: 3600  # 1 hour
popular_models:  # Fallback models if API fails
  - "llama-3.1-8b-instant"
  - "llama-3.1-70b-versatile" 
  - "mixtral-8x7b-32768"
  - "gemma2-9b-it"

litellm_provider_name: "groq"
model_prefix: "groq/"
```

**Ollama Provider Template:**
```yaml  
id: "ollama"
name: "Ollama"
description: "Run LLMs locally on your machine"
category: "local"
setup_difficulty: "medium"
documentation_url: "https://ollama.ai/docs/"

config_schema:
  api_base:
    type: "url"
    label: "Ollama Server URL"
    placeholder: "http://localhost:11434"
    required: true
    default_value: "http://localhost:11434"
    description: "URL of your Ollama server"

model_fetching: "dynamic"
model_endpoint: "/api/tags"  # Relative to api_base
model_list_cache_ttl: 300    # 5 minutes (local changes frequently)
popular_models:  # Fallback models if server unavailable
  - "llama3.2:latest"
  - "qwen2.5:latest" 
  - "deepseek-coder:latest"
  - "mistral-nemo:latest"

litellm_provider_name: "ollama"
```

**Together AI Provider Template:**
```yaml
id: "together"
name: "Together AI"  
description: "Fast inference for open-source models"
category: "cloud"
setup_difficulty: "easy"

config_schema:
  api_key:
    type: "password" 
    label: "Together AI API Key"
    required: true

model_fetching: "dynamic"
model_endpoint: "https://api.together.xyz/v1/models"
model_list_cache_ttl: 3600  # 1 hour
popular_models:  # Fallback models if API fails
  - "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
  - "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
  - "mistralai/Mixtral-8x7B-Instruct-v0.1"

litellm_provider_name: "together_ai"
model_prefix: "together_ai/"
```

**Fireworks AI Provider Template:**
```yaml
id: "fireworks"
name: "Fireworks AI"  
description: "Fast inference for production applications"
category: "cloud"
setup_difficulty: "easy"
documentation_url: "https://docs.fireworks.ai/"

config_schema:
  api_key:
    type: "password" 
    label: "Fireworks AI API Key"
    placeholder: "fw_..."
    required: true
    description: "Get your API key from fireworks.ai/api-keys"

model_fetching: "dynamic"
model_endpoint: "https://api.fireworks.ai/inference/v1/models"
model_list_cache_ttl: 3600  # 1 hour
popular_models:  # Fallback models if API fails
  - "accounts/fireworks/models/llama-v3p1-8b-instruct"
  - "accounts/fireworks/models/llama-v3p1-70b-instruct"
  - "accounts/fireworks/models/mixtral-8x7b-instruct"

litellm_provider_name: "fireworks_ai"
model_prefix: "fireworks_ai/"
```

### Dynamic Model Fetching Implementation

**Backend API Endpoints:**
```python
# New endpoints to add
GET /api/admin/llm/providers/{provider_id}/models
POST /api/admin/llm/providers/{provider_id}/refresh-models

# Implementation strategy
class ModelFetcher:
    async def fetch_models(self, provider: ProviderTemplate) -> List[str]:
        if provider.model_fetching == "dynamic":
            return await self._fetch_from_api(provider)
        elif provider.model_fetching == "static":
            return provider.popular_models or []
        else:  # manual
            return []  # User must input manually
    
    async def _fetch_from_api(self, provider: ProviderTemplate) -> List[str]:
        try:
            # Use provider's model_endpoint to fetch models
            # Cache results with provider.model_list_cache_ttl
            # Return parsed model names
        except Exception:
            # Fallback to popular_models if API fails
            return provider.popular_models or []
```

**Frontend Components:**
```typescript
// Model discovery component
interface ModelDiscoveryProps {
  provider: ProviderTemplate;
  onModelsSelected: (models: string[]) => void;
}

// Features:
// - Automatic model fetching when provider is configured
// - Loading states during API calls
// - Fallback to popular models on failure
// - Manual model input as backup
// - Real-time validation of model availability
```

**Caching Strategy:**
- **Groq/Together/Fireworks**: 1 hour cache (cloud providers, models don't change often)
- **Ollama**: 5 minutes cache (local server, models can be added/removed frequently)  
- **Cache invalidation**: Manual refresh button + TTL expiration
- **Error handling**: Always fallback to static popular models list

### Benefits of New Architecture

**For Users:**
- **Easier Discovery**: Browse 100+ providers by category 
- **Guided Setup**: Step-by-step wizard with validation
- **Better Documentation**: Contextual help and links
- **Faster Configuration**: Smart defaults and templates

**For Developers:**
- **Extensible**: Add new providers without code changes
- **Maintainable**: Centralized provider definitions
- **Testable**: Standardized configuration schema
- **Scalable**: Support for hundreds of providers

**For Enterprise:**
- **Bulk Setup**: Configure multiple providers at once
- **Compliance**: Built-in validation and security checks
- **Monitoring**: Provider health and performance tracking
- **Templates**: Pre-configured setups for common use cases

## üí° Updated Implementation Strategy

### Phase 1: Priority Providers (Focus Scope)
To avoid overwhelming task scope, implement built-in integration for these popular providers first:

1. **Ollama** (Local/Self-hosted)
   - 70% complete - missing provider definition
   - Dynamic model fetching via `/api/tags`
   - Popular for local development

2. **Groq** (Cloud - Ultra Fast)
   - Full implementation needed
   - Dynamic model fetching via Groq API
   - Known for speed and performance

3. **Together AI** (Cloud - Open Source Models)
   - Full implementation needed  
   - Dynamic model fetching via Together API
   - Good variety of open source models

4. **Fireworks AI** (Cloud - Fast Inference)
   - Full implementation needed
   - Dynamic model fetching capabilities
   - Enterprise-focused with good performance

### Phase 2: Preserve Custom Integration
- **KEEP existing "Custom LLM Provider" feature**
- Essential for future LiteLLM provider additions
- Allows manual configuration for any LiteLLM-supported provider
- Future-proofs the system as LiteLLM adds more integrations

### Dynamic Model Management Strategy
- **Primary**: Dynamic fetching from provider APIs
- **Fallback**: Static model list if API unavailable  
- **Cache**: TTL-based caching (5min-1hour depending on provider)
- **Real-time**: Models update when provider adds/removes them
- **Error Handling**: Graceful fallback to cached/static models

### Architecture Benefits
- **No Hardcoding**: Models fetched from provider APIs in real-time
- **Future-Proof**: New models automatically available
- **Resilient**: Fallback models if API fails
- **Performance**: Cached results with appropriate TTL
- **Extensible**: Custom integration preserved for edge cases
- **Scalable**: Template system supports unlimited providers