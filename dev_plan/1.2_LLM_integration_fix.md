# 1.2 LLM Integration Ad Blocker Fix

## Problem Statement

The Onyx LLM configuration wizard fails during the "Model Discovery" step with `net::ERR_BLOCKED_BY_CLIENT` errors caused by ad blockers detecting suspicious URL patterns in the provider model fetching endpoints.

### Root Cause Analysis

**Current Architecture Issues:**
- Frontend calls `/admin/llm/providers/{provider_id}/models` directly
- URL pattern contains "admin", "providers" keywords that trigger ad blocker heuristics
- Backend makes external API calls to `api.groq.com/openai/v1/models` during frontend requests
- No proxy layer to shield external API calls from browser context

**Ad Blocker Detection Patterns:**
- EasyList/EasyPrivacy filter lists block URLs containing `/admin/.*provider/`
- Third-party API integration patterns detected as tracking/analytics
- Dynamic model fetching interpreted as advertisement management

## Solution Strategy

Implement n8n-inspired backend proxy pattern with clean URL structure to eliminate ad blocker interference while maintaining full functionality.

## Current Architecture Analysis

### Frontend Flow
```typescript
// ModelDiscovery.tsx → ProviderTemplateAPI.ts
ProviderTemplateAPI.fetchProviderModels('groq') 
  ↓
fetch('/admin/llm/providers/groq/models')  // ← BLOCKED BY AD BLOCKER
```

### Backend Flow  
```python
# api.py
@admin_router.get("/providers/{provider_id}/models")
def fetch_provider_models(provider_id: str):
    # Direct external API call during request
    response = requests.get("https://api.groq.com/openai/v1/models")  # ← SUSPICIOUS
```

### Working Providers (No Issues)
- **OpenAI**: Uses static model list from `OPEN_AI_MODEL_NAMES`
- **Anthropic**: Uses static model list from `ANTHROPIC_MODEL_NAMES`  
- **Azure/Bedrock**: No dynamic model fetching required

### Problematic Providers (Blocked)
- **Groq**: Dynamic fetching via `model_endpoint: "https://api.groq.com/openai/v1/models"`
- **Ollama**: Dynamic fetching via relative endpoint `/api/tags`
- **Together AI**: Dynamic fetching via `model_endpoint: "https://api.together.xyz/v1/models"`
- **Fireworks AI**: Dynamic fetching via `model_endpoint: "https://api.fireworks.ai/inference/v1/models"`

## Proposed Solution

### Phase 1: Clean Proxy URLs

Replace ad blocker-triggering URLs with clean proxy endpoints:

**Before (Problematic):**
```
GET /admin/llm/providers/groq/models
```

**After (Clean):**
```
GET /api/llm-models?provider=groq
GET /api/models/discovery?provider=groq  
```

### Phase 2: Backend Proxy Layer

Implement n8n-style backend proxy with caching:

```python
@api_router.get("/llm-models")
async def get_llm_models(provider: str):
    """Clean proxy endpoint for model discovery"""
    # Background model fetching with caching
    # No direct frontend → external API exposure
```

### Phase 3: Intelligent Fallback

Maintain backward compatibility with graceful fallback:

```typescript
// Frontend with automatic fallback
async fetchModels(provider: string) {
    try {
        // Try new clean proxy endpoint
        return await fetch(`/api/llm-models?provider=${provider}`);
    } catch {
        // Fallback to original endpoint for non-ad-blocked environments
        return await fetch(`/admin/llm/providers/${provider}/models`);
    }
}
```

## Implementation Plan

### Files to Modify

#### Backend Changes
1. **`onyx/server/manage/llm/api.py`**
   - Add new clean proxy endpoints
   - Implement background model caching
   - Maintain existing endpoints for compatibility

2. **`onyx/llm/provider_templates.py`** 
   - Add proxy configuration support
   - Implement model caching mechanisms

3. **`onyx/main.py`**
   - Register new API routes in main router
   - Configure proxy middleware

#### Frontend Changes  
1. **`web/src/lib/api/providerTemplates.ts`**
   - Add clean proxy endpoint calls
   - Implement intelligent fallback logic
   - Enhanced error handling with user guidance

2. **`web/src/components/llm/ModelDiscovery.tsx`**
   - Update to handle new response format
   - Add better loading and error states

### New URL Structure

#### Clean Proxy Endpoints (Ad Blocker Safe)
```
GET /api/llm-models?provider=groq           # Model discovery
GET /api/llm-models/refresh?provider=groq   # Force refresh  
GET /api/llm-providers                       # Available providers
GET /api/llm-providers/templates             # Provider templates
```

#### Existing Endpoints (Maintained for Compatibility)
```
GET /admin/llm/built-in/options             # Provider templates (working)
GET /admin/llm/providers/{id}/models         # Model discovery (blocked)
POST /admin/llm/providers/{id}/refresh-models # Force refresh (blocked)
```

## Technical Implementation Details

### Backend Proxy Service

```python
class LLMModelProxyService:
    """Clean proxy service for LLM model discovery"""
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 3600  # 1 hour
    
    async def get_models(self, provider_name: str) -> dict:
        """Get models via background proxy with caching"""
        cache_key = f"models:{provider_name}"
        
        # Check cache first
        if self.is_cached(cache_key):
            return self.cache[cache_key]
        
        # Background fetch from external API
        models = await self._fetch_models_background(provider_name)
        
        # Cache results
        self.cache[cache_key] = {
            'models': models,
            'cached': False,
            'timestamp': int(time.time()),
            'ttl': self.cache_ttl
        }
        
        return self.cache[cache_key]
    
    async def _fetch_models_background(self, provider_name: str) -> list:
        """Background fetching shielded from frontend"""
        provider = self._get_provider_config(provider_name)
        
        if not provider.model_endpoint:
            return self._get_static_models(provider_name)
        
        # Proxy external API call
        async with httpx.AsyncClient() as client:
            headers = self._get_auth_headers(provider_name)
            response = await client.get(provider.model_endpoint, headers=headers)
            
            if response.status_code == 200:
                return self._parse_models_response(response.json())
            
        # Fallback to static models
        return self._get_static_models(provider_name)
```

### Frontend Intelligent Fallback

```typescript
export class ProviderTemplateAPI {
    private static async fetchWithFallback(url: string, fallbackUrl?: string): Promise<Response> {
        try {
            // Try clean proxy endpoint first
            const response = await fetch(url);
            if (response.ok) return response;
        } catch (error) {
            // Ad blocker or network issue
            console.warn('Primary endpoint failed, trying fallback:', error.message);
        }
        
        if (fallbackUrl) {
            try {
                return await fetch(fallbackUrl);
            } catch (error) {
                // Enhanced error message for ad blocker detection
                if (error instanceof TypeError && error.message.includes('Failed to fetch')) {
                    throw new Error(`Model discovery blocked. Please disable ad blocker for localhost or try incognito mode.`);
                }
                throw error;
            }
        }
        
        throw new Error('All endpoints failed');
    }
    
    static async fetchProviderModels(providerId: string): Promise<ModelFetchResponse> {
        // Clean proxy endpoint (ad blocker safe)
        const proxyUrl = `${API_BASE}/api/llm-models?provider=${providerId}`;
        // Fallback endpoint (original)
        const fallbackUrl = `${API_BASE}/admin/llm/providers/${providerId}/models`;
        
        const response = await this.fetchWithFallback(proxyUrl, fallbackUrl);
        return await response.json();
    }
}
```

## Benefits of This Solution

### 1. Ad Blocker Compatibility
- **Clean URLs**: No "admin/provider" patterns that trigger blockers
- **Backend Proxy**: External API calls invisible to browser
- **Safe Patterns**: Follows n8n's proven anti-blocking strategy

### 2. Performance Improvements  
- **Intelligent Caching**: 1-hour TTL for model lists
- **Background Fetching**: No frontend blocking on external APIs
- **Reduced Load**: Cached responses for repeated requests

### 3. Enhanced User Experience
- **Automatic Fallback**: Works in all environments
- **Better Error Messages**: Clear guidance for ad blocker issues  
- **Progressive Enhancement**: Graceful degradation

### 4. Maintainability
- **Backward Compatible**: Existing endpoints preserved
- **Clean Architecture**: Separation of concerns
- **Future Proof**: Easy to extend for new providers

### 5. Security Benefits
- **API Key Protection**: No frontend exposure of credentials
- **Request Validation**: Backend validates all external calls
- **Error Isolation**: External API failures handled gracefully

## Testing Strategy

### Unit Tests
- Backend proxy service functionality
- Frontend fallback logic
- Cache management and TTL handling
- Error scenarios and recovery

### Integration Tests  
- End-to-end model discovery flow
- Ad blocker simulation testing
- Network failure simulation
- Performance benchmarking

### User Acceptance Testing
- Test with popular ad blockers (uBlock Origin, AdBlock Plus)
- Verify fallback behavior in various browsers
- Confirm UI/UX improvements

## Migration Path

### Phase 1: Infrastructure (Week 1)
1. Implement backend proxy service
2. Add clean proxy endpoints
3. Create comprehensive test suite

### Phase 2: Frontend Updates (Week 1)  
1. Update API client with fallback logic
2. Enhanced error handling and messaging
3. UI improvements for better user experience

### Phase 3: Testing & Rollout (Week 1)
1. Comprehensive testing with ad blockers
2. Performance validation
3. Documentation updates

### Phase 4: Monitoring (Ongoing)
1. Monitor error rates and fallback usage
2. Performance metrics tracking
3. User feedback collection

## Risk Mitigation

### Technical Risks
- **Cache Invalidation**: Implement TTL and manual refresh capabilities
- **Endpoint Compatibility**: Maintain existing endpoints during transition
- **External API Changes**: Robust error handling and fallback strategies

### User Experience Risks  
- **Migration Disruption**: Seamless fallback ensures no user impact
- **Performance Regression**: Caching improves performance over current implementation
- **Ad Blocker False Positives**: Clean URL patterns prevent future blocking

### Operational Risks
- **Monitoring**: Implement comprehensive logging and metrics
- **Rollback Plan**: Easy rollback via feature flags or endpoint switching
- **Documentation**: Clear migration guide and troubleshooting docs

## Success Metrics

### Primary KPIs
- **Ad Blocker Success Rate**: >95% success rate with ad blockers enabled
- **Model Discovery Completion**: >98% success rate for provider setup
- **Error Reduction**: <1% "Failed to fetch" errors in production

### Secondary KPIs  
- **Performance**: <500ms response time for cached models
- **User Satisfaction**: Reduced support tickets related to setup issues
- **Developer Experience**: Cleaner codebase and better maintainability

## Conclusion

This solution directly addresses the ad blocker interference issue while significantly improving the overall architecture, performance, and user experience of the LLM provider integration system. The n8n-inspired approach provides a robust, scalable foundation for future provider integrations while maintaining full backward compatibility.

The implementation follows Onyx's existing patterns and conventions, ensuring smooth integration with the current codebase and development workflow.