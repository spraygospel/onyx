# 1.1 Built-in LLM Integration - Todo List

Development task breakdown for extensible LLM provider architecture implementation.

## 📋 Phase 0: Create ALL Test Scripts First (TDD Setup)

**Following CLAUDE.md TDD principles: Write ALL tests FIRST as specifications**

### Backend Test Scripts (Create ALL Before Any Implementation)
- [ ] **Provider Template Tests**
  - Location: `tests/unit/backend/llm/test_provider_templates.py`
  - Test: ProviderTemplate validation, schema validation, required fields
  - Test: Provider category classification and difficulty levels
  - Test: LiteLLM provider name mapping validation

- [ ] **Model Fetcher Tests** 
  - Location: `tests/unit/backend/llm/test_model_fetcher.py`
  - Test: Dynamic API fetching for each provider (mocked)
  - Test: TTL caching behavior and cache invalidation
  - Test: Fallback to static models when API fails
  - Test: Error handling for network timeouts and invalid responses

- [ ] **Provider API Integration Tests**
  - Location: `tests/integration/backend/llm/test_provider_apis.py`
  - Test: Real API calls to Groq, Together AI, Fireworks AI (with test API keys)
  - Test: Ollama local server integration (with test server)
  - Test: Model list parsing for each provider format
  - Test: Authentication and rate limiting handling

- [ ] **LLM Provider Options Tests**
  - Location: `tests/unit/backend/llm/test_llm_provider_options.py`
  - Test: Updated `fetch_available_well_known_llms()` with new providers
  - Test: Backward compatibility with existing 5 providers
  - Test: Provider template integration with existing interfaces

- [ ] **API Endpoint Tests**
  - Location: `tests/integration/backend/api/test_llm_provider_management.py`
  - Test: `GET /api/admin/llm/providers/{provider_id}/models` endpoint
  - Test: `POST /api/admin/llm/providers/{provider_id}/refresh-models` endpoint
  - Test: Error responses and status codes
  - Test: Authentication and authorization for admin endpoints

### Frontend Test Scripts (Create ALL Before Any Implementation)
- [ ] **Provider Template Interface Tests**
  - Location: `tests/unit/frontend/llm/interfaces.test.ts`
  - Test: TypeScript interface validation for ProviderTemplate
  - Test: Config schema field types and validation
  - Test: Model fetching configuration options

- [ ] **Provider Card Component Tests**
  - Location: `tests/unit/frontend/components/ProviderCard.test.tsx`
  - Test: Rendering with different provider configurations
  - Test: Category badge display and setup difficulty indicators
  - Test: Click handlers and provider selection
  - Test: Loading and error states

- [ ] **Dynamic Form Generator Tests**
  - Location: `tests/unit/frontend/components/DynamicForm.test.tsx`
  - Test: Form generation from provider config_schema
  - Test: Different field types (text, password, url, select, file)
  - Test: Form validation and error messages
  - Test: Real-time validation and help text display

- [ ] **Model Discovery Component Tests**
  - Location: `tests/unit/frontend/components/ModelDiscovery.test.tsx`
  - Test: API-based model fetching with loading states
  - Test: Fallback to static models on API failure
  - Test: Manual model input and validation
  - Test: Model caching and refresh functionality

- [ ] **Configuration Wizard Tests**
  - Location: `tests/unit/frontend/components/ConfigWizard.test.tsx`
  - Test: 4-step wizard navigation and state management
  - Test: Provider selection, configuration, model selection, testing
  - Test: Form data persistence across steps
  - Test: Error handling and validation at each step

### Unit Test Scripts Only (E2E Tests at Final Phase)
**Note: E2E tests will be created and run AFTER all features are implemented**

### Performance Test Scripts (Create ALL Before Implementation)
- [ ] **API Performance Tests**
  - Location: `tests/performance/backend/api_performance.test.py`
  - Test: Model fetching response times (<5 seconds)
  - Test: Concurrent API calls handling
  - Test: Cache performance and hit rates (>80%)
  - Test: Memory usage during model fetching

- [ ] **Frontend Performance Tests**
  - Location: `tests/performance/frontend/ui_performance.spec.ts`
  - Test: Provider list rendering (<2 seconds)
  - Test: Dynamic form generation (<1 second)
  - Test: Search and filter responsiveness (<500ms)
  - Test: Configuration wizard flow performance

### Test Data and Fixtures (Create ALL Before Implementation)
- [ ] **Mock Provider Templates**
  - Location: `tests/fixtures/provider_templates.json`
  - Mock data for Groq, Ollama, Together AI, Fireworks AI
  - Mock API responses for model fetching
  - Test configuration schemas and field validation

- [ ] **Mock API Responses**
  - Location: `tests/fixtures/api_responses/`
  - Mock Groq API model list response
  - Mock Together AI API model list response  
  - Mock Fireworks AI API model list response
  - Mock Ollama `/api/tags` response

- [ ] **Test Environment Setup**
  - Location: `tests/setup/test_llm_providers.py`
  - Test database setup with provider configurations
  - Mock external API services for consistent testing
  - Test user permissions and authentication

## 📋 Phase 1: Fitur A - Provider Template System

### 1A. Implement + Test Provider Template Interface
- [ ] **Create ProviderTemplate Interface** 
  - Define interfaces for extensible provider configuration
  - Location: `backend/onyx/llm/provider_templates.py`
  - Include: id, name, description, category, config_schema, model_fetching, etc.

- [ ] **Run Provider Template Tests**
  - Execute: `tests/unit/backend/llm/test_provider_templates.py`
  - Verify: Template validation, schema validation, required fields
  - Fix any failing tests before proceeding

- [ ] **Create Frontend Provider Template Types**
  - Define TypeScript interfaces matching backend templates  
  - Location: `web/src/app/admin/configuration/llm/interfaces.ts`
  - Include all configuration schemas and validation types

- [ ] **Run Frontend Interface Tests**
  - Execute: `tests/unit/frontend/llm/interfaces.test.ts`
  - Verify: TypeScript interface validation, config schema field types
  - Fix any failing tests before proceeding

**✅ Phase 1 Complete Criteria: All Provider Template tests passing**

---

## 📋 Phase 2: Fitur B - Dynamic Model Fetcher

### 2A. Implement + Test Model Fetching System
- [ ] **Create Dynamic Model Fetcher**
  - Create ModelFetcher class with dynamic API fetching
  - Location: `backend/onyx/llm/model_fetcher.py` 
  - Features: API fetching, TTL caching, fallback to static models

- [ ] **Run Model Fetcher Tests**
  - Execute: `tests/unit/backend/llm/test_model_fetcher.py`
  - Verify: Dynamic API fetching, TTL caching, fallback mechanisms
  - Fix any failing tests before proceeding

- [ ] **Add New API Endpoints**
  - `GET /api/admin/llm/providers/{provider_id}/models`
  - `POST /api/admin/llm/providers/{provider_id}/refresh-models`
  - Location: `backend/onyx/server/manage/llm/`

- [ ] **Run API Endpoint Tests**
  - Execute: `tests/integration/backend/api/test_llm_provider_management.py`
  - Verify: Endpoint responses, error handling, authentication
  - Fix any failing tests before proceeding

**✅ Phase 2 Complete Criteria: All Model Fetcher tests passing**

---

## 📋 Phase 3: Fitur C - Provider Card UI Components

### 3A. Implement + Test UI Components
- [ ] **Build Provider Card Component**
  - Visual provider cards with logo, difficulty, setup preview
  - Location: `web/src/app/admin/configuration/llm/components/ProviderCard.tsx`
  - Features: category badges, setup difficulty indicators, provider logos

- [ ] **Run Provider Card Tests**
  - Execute: `tests/unit/frontend/components/ProviderCard.test.tsx`
  - Verify: Rendering, category badges, click handlers, loading states
  - Fix any failing tests before proceeding

- [ ] **Create Dynamic Form Generator**
  - Generate forms automatically from provider template config_schema
  - Location: `web/src/app/admin/configuration/llm/components/DynamicForm.tsx`
  - Support: text, password, url, select, file, textarea field types

- [ ] **Run Dynamic Form Tests**
  - Execute: `tests/unit/frontend/components/DynamicForm.test.tsx`
  - Verify: Form generation, field types, validation, error messages
  - Fix any failing tests before proceeding

**✅ Phase 3 Complete Criteria: All UI Component tests passing**

---

## 📋 Phase 4: Fitur D - Model Discovery Component

### 4A. Implement + Test Model Discovery
- [ ] **Create Model Discovery Component**
  - Real-time model fetching with loading states
  - Location: `web/src/app/admin/configuration/llm/components/ModelDiscovery.tsx`
  - Features: API fetching, fallback models, manual input, validation

- [ ] **Run Model Discovery Tests**
  - Execute: `tests/unit/frontend/components/ModelDiscovery.test.tsx`
  - Verify: API fetching, fallback behavior, manual input, caching
  - Fix any failing tests before proceeding

**✅ Phase 4 Complete Criteria: All Model Discovery tests passing**

---

## 📋 Phase 5: Fitur E - Provider Registry Integration

### 5A. Implement + Test Provider Registry Updates
- [ ] **Update Provider Registry System**
  - Modify `fetch_available_well_known_llms()` to use templates
  - Location: `backend/onyx/llm/llm_provider_options.py`
  - Maintain backward compatibility with existing 5 providers

- [ ] **Run Provider Registry Tests**
  - Execute: `tests/unit/backend/llm/test_llm_provider_options.py`
  - Verify: Updated function, backward compatibility, template integration
  - Fix any failing tests before proceeding

**✅ Phase 5 Complete Criteria: All Provider Registry tests passing**

---

## 📋 Phase 6: Fitur F - Ollama Provider Integration

### 6A. Implement + Test Ollama Integration (70% Complete)
- [ ] **Complete Ollama Provider Definition**
  - Add OLLAMA_PROVIDER_NAME and template to `llm_provider_options.py`
  - Define popular models: llama3.2, qwen2.5, deepseek-coder, mistral-nemo
  - Configure dynamic model fetching via `/api/tags` endpoint

- [ ] **Run Ollama Integration Tests**
  - Execute: `tests/integration/backend/llm/test_provider_apis.py` (Ollama section)
  - Verify: `/api/tags` endpoint parsing, fallback models, 5-minute cache TTL
  - Fix any failing tests before proceeding

**✅ Phase 6 Complete Criteria: Ollama integration tests passing**

---

## 📋 Phase 7: Fitur G - Groq Provider Integration

### 7A. Implement + Test Groq Integration
- [ ] **Add Groq Provider Definition**
  - Create GROQ_PROVIDER_NAME and template
  - Configure dynamic model fetching via Groq API
  - Set up 1-hour cache TTL for cloud provider

- [ ] **Implement Groq API Integration**
  - Model fetching from `https://api.groq.com/openai/v1/models`
  - Parse response format and extract model names
  - Handle authentication with API key validation

- [ ] **Run Groq Integration Tests**
  - Execute: `tests/integration/backend/llm/test_provider_apis.py` (Groq section)
  - Verify: API connectivity, model parsing, fallback models
  - Fix any failing tests before proceeding

**✅ Phase 7 Complete Criteria: Groq integration tests passing**

---

## 📋 Phase 8: Fitur H - Together AI Provider Integration

### 8A. Implement + Test Together AI Integration
- [ ] **Add Together AI Provider Definition**
  - Create TOGETHER_PROVIDER_NAME and template
  - Configure dynamic model fetching via Together API
  - Set up 1-hour cache TTL for cloud provider

- [ ] **Implement Together AI API Integration**
  - Model fetching from `https://api.together.xyz/v1/models`
  - Parse Together-specific response format
  - Handle API key authentication and rate limiting

- [ ] **Run Together AI Integration Tests**
  - Execute: `tests/integration/backend/llm/test_provider_apis.py` (Together AI section)
  - Verify: API connectivity, model parsing, model name formatting
  - Fix any failing tests before proceeding

**✅ Phase 8 Complete Criteria: Together AI integration tests passing**

---

## 📋 Phase 9: Fitur I - Fireworks AI Provider Integration

### 9A. Implement + Test Fireworks AI Integration  
- [ ] **Add Fireworks AI Provider Definition**
  - Create FIREWORKS_PROVIDER_NAME and template
  - Configure dynamic model fetching via Fireworks API
  - Set up 1-hour cache TTL for cloud provider

- [ ] **Implement Fireworks AI API Integration**
  - Model fetching from `https://api.fireworks.ai/inference/v1/models`
  - Parse Fireworks-specific response format
  - Handle API key authentication and error handling

- [ ] **Run Fireworks AI Integration Tests**
  - Execute: `tests/integration/backend/llm/test_provider_apis.py` (Fireworks AI section)
  - Verify: API connectivity, model parsing, enterprise-grade reliability
  - Fix any failing tests before proceeding

**✅ Phase 9 Complete Criteria: Fireworks AI integration tests passing**

---

## 📋 Phase 10: Fitur J - Configuration Wizard

### 10A. Implement + Test Configuration Wizard
- [ ] **Create 4-Step Configuration Wizard**
  - Step 1: Provider Selection (categories, search, recommendations)
  - Step 2: Basic Configuration (dynamic form generation)
  - Step 3: Model Selection (dynamic fetching, fallback, manual input)
  - Step 4: Test Connection (validation, error handling)

- [ ] **Run Configuration Wizard Tests**
  - Execute: `tests/unit/frontend/components/ConfigWizard.test.tsx`
  - Verify: 4-step navigation, state management, form data persistence
  - Fix any failing tests before proceeding

**✅ Phase 10 Complete Criteria: Configuration Wizard tests passing**

---

## 📋 Phase 11: Fitur K - Provider Categorization & Search

### 11A. Implement + Test Provider Categorization
- [ ] **Implement Provider Categorization**
  - Popular Cloud: OpenAI, Anthropic, Groq, Together AI, Fireworks AI
  - Enterprise: Azure OpenAI, AWS Bedrock, GCP Vertex AI
  - Local/Self-hosted: Ollama, LM Studio, LocalAI
  - Custom Integration: Manual LiteLLM provider configuration

- [ ] **Add Search and Filter Functionality**
  - Provider search by name and description
  - Category filtering with visual indicators
  - Setup difficulty filtering (easy/medium/hard)
  - Provider recommendation engine

- [ ] **Run Categorization Tests**
  - Execute: Frontend tests for search and filter functionality
  - Verify: Search performance, category filtering, recommendation engine
  - Fix any failing tests before proceeding

**✅ Phase 11 Complete Criteria: Provider categorization tests passing**

---

## 📋 Phase 12: Fitur L - Error Handling & Validation

### 12A. Implement + Test Error Handling System
- [ ] **Implement Comprehensive Error Handling**
  - API timeout handling with graceful fallbacks
  - Network error recovery with retry logic
  - Invalid API key detection and user feedback
  - Model availability validation

- [ ] **Add Real-time Validation**
  - API key format validation
  - Connection testing during setup
  - Model availability verification
  - Configuration completeness checking

- [ ] **Create Loading and Status Components**
  - Loading states for API calls
  - Progress indicators for multi-step processes
  - Status messages for success/error feedback
  - Retry mechanisms for failed operations

- [ ] **Run Error Handling Tests**
  - Execute: All error handling related unit tests
  - Verify: Graceful fallbacks, user feedback, validation logic
  - Fix any failing tests before proceeding

**✅ Phase 12 Complete Criteria: All error handling tests passing**

---

## 📋 Phase 13: Fitur M - Custom Integration Preservation

### 13A. Implement + Test Custom Integration
- [ ] **Maintain Existing Custom LLM Provider Feature**
  - Ensure CustomLLMProviderUpdateForm remains functional
  - Test integration with new provider template system
  - Validate backward compatibility with existing setups

- [ ] **Enhance Custom Integration Experience**
  - Improve form validation for manual configurations
  - Add LiteLLM provider documentation links
  - Include configuration examples and templates
  - Support for future LiteLLM provider additions

- [ ] **Run Custom Integration Tests**
  - Execute: Tests for CustomLLMProviderUpdateForm functionality
  - Verify: Backward compatibility, form validation, documentation links
  - Fix any failing tests before proceeding

**✅ Phase 13 Complete Criteria: Custom integration tests passing**

---

## 📋 Phase 14: FINAL - End-to-End Testing

**🎯 All features implemented - now test complete user workflows**

### 14A. Create & Run Complete E2E Test Suite
- [ ] **Create Complete Provider Setup E2E Tests**
  - Location: `tests/e2e/llm_provider_setup.spec.ts`
  - Test: Complete Ollama provider setup workflow
  - Test: Complete Groq provider setup workflow  
  - Test: Complete Together AI provider setup workflow
  - Test: Complete Fireworks AI provider setup workflow
  - Test: Error scenarios and recovery

- [ ] **Create Provider Management E2E Tests**
  - Location: `tests/e2e/llm_provider_management.spec.ts`
  - Test: Provider listing and categorization
  - Test: Search and filter functionality
  - Test: Provider editing and deletion
  - Test: Custom integration preservation

- [ ] **Create Model Selection E2E Tests**
  - Location: `tests/e2e/llm_model_selection.spec.ts`
  - Test: Dynamic model fetching in browser
  - Test: Model selection and default model setup
  - Test: Model availability validation
  - Test: Fallback model behavior

- [ ] **Run Complete E2E Test Suite**
  - Execute: `npm run test:e2e` or `npx playwright test`
  - Verify: All user workflows work end-to-end
  - Fix any failing E2E tests

- [ ] **Run Performance Tests**
  - Execute: `tests/performance/backend/api_performance.test.py`
  - Execute: `tests/performance/frontend/ui_performance.spec.ts`
  - Verify: Performance targets met (<5s API, <2s UI, <500ms search)
  - Optimize if performance targets not met

**✅ FINAL SUCCESS CRITERIA: All E2E tests passing + Performance targets met**

---

## 📋 Optional Future Enhancements (Post-Launch)

### Monitoring and Health Checks (Optional)
- [ ] **Implement Provider Health Monitoring**
  - Connection status tracking
  - Model availability monitoring
  - Performance metrics collection
  - Alert system for provider issues

### Enterprise Features (Optional)
- [ ] **Bulk Provider Setup**
  - Import/export provider configurations
  - Batch provider configuration
  - Template-based setup for common use cases
  - Enterprise compliance validation

## 📋 Success Criteria Summary

### ✅ Final Acceptance Criteria
- **All 14 Phases Complete**: Each phase tests pass before moving to next
- **4 Priority Providers Working**: Ollama, Groq, Together AI, Fireworks AI fully functional
- **Dynamic Model Fetching**: Real-time API fetching with fallback mechanisms
- **Custom Integration Preserved**: Existing custom LLM provider feature maintained
- **E2E Tests Passing**: All user workflows tested and working
- **Performance Targets Met**: <5s API, <2s UI, <500ms search

### 📊 Quality Metrics
- **Unit Test Coverage**: >80% for all new components  
- **Integration Tests**: All 4 provider APIs tested with real connections
- **E2E Test Coverage**: Complete user workflows from provider selection to usage
- **Error Handling**: Graceful degradation with clear user feedback
- **Backward Compatibility**: Existing configurations continue working

---

## 📝 Notes

**Estimated Timeline**: 3-4 weeks for complete implementation
**Priority Order**: Phase 0 (All Tests) → Phase 1-13 (Fitur A-M, each with test-implement-verify cycle) → Phase 14 (Final E2E)
**Dependencies**: Phase 0 must be completed first, then each subsequent phase tests pass before moving to next phase
**Risk Factors**: Provider API changes, rate limiting, authentication issues

**TDD Workflow per Phase**:
1. Write tests for specific feature (already done in Phase 0)
2. Implement feature to make tests pass
3. Verify all tests pass before moving to next phase
4. Refactor if needed while keeping tests green

**Team Coordination**: 
- Phase 0 (all tests) must be completed first by entire team
- Phases 1-5 (core infrastructure) should be sequential (backend dependencies)
- Phases 6-9 (provider integrations) can be done in parallel by different team members
- Phases 10-13 (UI/UX features) can run parallel to provider work
- Phase 14 (E2E) requires all previous phases complete