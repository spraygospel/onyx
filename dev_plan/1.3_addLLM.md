# Development Plan 1.3: Model Discovery → Model Add Enhancement

## Problem Statement

Current Model Discovery step in LLM provider setup suffers from:
1. **Ad blocker interference** - Dynamic model fetching URLs get blocked by EasyList patterns
2. **Network reliability issues** - Dependency on external API calls during setup 
3. **User friction** - Complex model selection from long lists
4. **Setup complexity** - Multiple API calls required for discovery + selection

## Solution Overview

Transform **Model Discovery** → **Model Add** with manual input approach:
- User manually types model names they want to use
- Each model gets tested individually for connection before adding
- Follow OpenAI's static model pattern (no ad blocker issues)
- Cleaner UX with progressive model addition

## Current Analysis

### OpenAI Integration Pattern (Works without ad blocker issues)
```typescript
// backend/onyx/llm/llm_provider_options.py:52-81
OPEN_AI_MODEL_NAMES = [
    "gpt-5", "gpt-5-mini", "o1", "gpt-4o", "gpt-4o-mini", 
    "gpt-3.5-turbo", // ... static list
]
```

**Key Differences:**
- ✅ **OpenAI**: Static model list, no dynamic fetching, no ad blocker conflicts
- ❌ **Groq**: Dynamic model discovery via `/api/llm-models?provider=Groq` → blocked by ad blockers

### Current Wizard Flow
```
Step 1: Configuration (API Key, Base URL)
Step 2: Test Connection  
Step 3: Model Discovery ← PROBLEM AREA
```

### Existing Manual Model Component
`ModelConfigurationField.tsx` already supports manual model entry:
```typescript
// Lines 42-47: Manual model name input
<TextFormField
  name={`${name}[${index}].name`}
  placeholder={`model-name-${index + 1}`}
/>
```

## Technical Design

### 1. Backend Changes

#### File: `backend/onyx/llm/llm_provider_options.py`
**Modification: Add test connection method for individual models**

```python
# Lines 407-420: Add to WellKnownLLMProviderDescriptor
def test_model_connection(self, model_name: str, config: dict) -> dict:
    """Test connection to specific model with given configuration"""
    try:
        # Use LiteLLM or provider-specific client to test model
        # Return {success: bool, error?: str, model_info?: dict}
        pass
    except Exception as e:
        return {"success": False, "error": str(e)}
```

#### File: `backend/onyx/server/manage/llm/api.py`
**Addition: New endpoint for model testing**

```python
@proxy_router.post("/test-model-connection")
def test_model_connection_endpoint(
    provider_id: str,
    model_name: str,
    configuration: dict,
    _: User | None = Depends(current_admin_user),
) -> dict[str, Any]:
    """Test connection to specific model for a provider."""
    # Implementation similar to existing test endpoints
    # But focused on individual model testing
```

### 2. Frontend Changes

#### File: `web/src/components/llm/ConfigurationWizard.tsx`
**Modification: Change step 3 title and description**

```typescript
// Lines 68-72: Update wizard step
{
  id: 'models',
  title: 'Add Models', // ← Changed from 'Model Discovery'
  description: 'Add and verify your models', // ← Changed
  icon: Plus, // ← Changed from Zap
}
```

#### File: `web/src/components/llm/ModelAdd.tsx` 
**New Component: Replace ModelDiscovery.tsx**

```typescript
interface ModelAddProps {
  provider: ProviderTemplate;
  apiConfig: Record<string, string>;
  selectedModels: string[];
  onModelsChanged: (models: string[]) => void;
}

interface ModelEntry {
  name: string;
  status: 'pending' | 'testing' | 'verified' | 'error';
  error?: string;
  testResult?: {success: boolean, model_info?: any};
}
```

### 3. UI/UX Design Changes

#### Current Model Discovery UI:
```
┌─────────────────────────────────────┐
│ Model Discovery          [Refresh] │
├─────────────────────────────────────┤
│ ⚡ Fast Models (2)                  │
│ ☑ whisper-large-v3-turbo           │
│ ☐ llama-3.1-8b-instant             │
│                                     │
│ 🔥 Powerful Models (5)              │
│ ☐ llama-3.3-70b-versatile          │
│ ☐ deepseek-r1-distill-llama-70b    │
│ ...                                 │
└─────────────────────────────────────┘
```

#### New Model Add UI:
```
┌─────────────────────────────────────┐
│ Add Models                          │
├─────────────────────────────────────┤
│ [Input: model-name]     [Test Add]  │
├─────────────────────────────────────┤
│ Added Models:                       │
│                                     │
│ ✅ llama-3.3-70b-versatile    [×]   │
│ 🧪 testing: mixtral-8x7b-32768      │
│ ❌ invalid-model-name          [×]   │
│    └─ Error: Model not found        │
│                                     │
│ [+ Add Another Model]               │
└─────────────────────────────────────┘
```

## Implementation Plan

### Phase 1: Backend Foundation
1. **Add model testing endpoint** (`/api/test-model-connection`)
2. **Enhance provider descriptor** with individual model testing capability
3. **Update proxy service** to support per-model validation

### Phase 2: Frontend Component
1. **Create ModelAdd.tsx** component replacing ModelDiscovery.tsx
2. **Update ConfigurationWizard.tsx** to use ModelAdd component
3. **Add testing states and UI feedback** for individual model verification

### Phase 3: Integration & Testing
1. **Update provider templates** to use new model addition flow
2. **Test with all provider types** (Groq, Ollama, Together AI, etc.)
3. **Ensure backward compatibility** with existing static providers

### Phase 4: UX Polish
1. **Add model suggestions** (popular models dropdown/autocomplete)
2. **Batch model testing** (add multiple models at once)
3. **Model validation feedback** (detailed error messages)

## File Changes Required

### Backend Files
- `backend/onyx/llm/llm_provider_options.py` ← Enhance descriptor
- `backend/onyx/server/manage/llm/api.py` ← Add test endpoint
- `backend/tests/unit/onyx/llm/test_model_testing.py` ← New test file

### Frontend Files
- `web/src/components/llm/ModelAdd.tsx` ← **New component**
- `web/src/components/llm/ConfigurationWizard.tsx` ← Update imports/usage
- `web/src/components/llm/ModelDiscovery.tsx` ← **Remove/deprecate**
- `web/src/hooks/useModelTesting.ts` ← **New hook**
- `web/src/lib/api/providerTemplates.ts` ← Add test model API call

## Benefits

### Technical Benefits
✅ **No ad blocker conflicts** - No dynamic model discovery URLs  
✅ **Better reliability** - No dependency on external provider APIs during setup  
✅ **Simpler architecture** - Manual input + validation vs complex discovery logic  
✅ **Faster setup** - Users only add models they actually need  

### User Experience Benefits  
✅ **More intuitive** - Users know which models they want to use  
✅ **Immediate feedback** - Test each model as it's added  
✅ **Error clarity** - Specific error messages per model  
✅ **Progressive setup** - Add models one by one vs overwhelming selection  

### Maintenance Benefits
✅ **Less external dependencies** - No reliance on provider model listing APIs  
✅ **Easier testing** - Mock individual model tests vs complex discovery flows  
✅ **Better error handling** - Granular per-model error states  

## Success Metrics

- ✅ Zero ad blocker conflicts during setup
- ✅ <2 second model verification per model  
- ✅ Clear error messages for invalid models
- ✅ Backward compatibility with existing providers
- ✅ User can complete setup with manual model input

## Risk Assessment

### Low Risk
- UI changes (well-contained component replacement)
- Backend endpoint addition (follows existing patterns)

### Medium Risk  
- Provider template integration (needs testing across all providers)
- Backward compatibility (ensure existing setups still work)

### Mitigation
- Feature flag for new vs old model discovery
- Comprehensive testing with all provider types
- Progressive rollout starting with Groq provider

## Implementation Timeline

**Week 1:** Backend foundation + API endpoint  
**Week 2:** Frontend ModelAdd component  
**Week 3:** Integration testing + bug fixes  
**Week 4:** UX polish + documentation

## Conclusion

This approach eliminates ad blocker conflicts by moving away from dynamic model discovery to a user-driven manual model addition process. Following OpenAI's successful static model pattern, we provide a more reliable and intuitive setup experience.

The solution addresses the root cause (ad blocker conflicts) while improving user experience through progressive model addition with immediate validation feedback.